{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553UZ1iuVOV7"
      },
      "source": [
        "# TP4, INF8225 2025, Projet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjWK-m8VOV8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XSGJpgj7VOV8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install numpy\n",
        "%pip install swig\n",
        "%pip install box2d\n",
        "%pip install pygame\n",
        "%pip install gymnasium\n",
        "%pip install \"gymnasium[box2d]\"\n",
        "%pip install matplotlib\n",
        "%pip install wandb\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fcRnqGzMVOV8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "import os\n",
        "import wandb\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "from collections import namedtuple, deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fvb3eqVOV8"
      },
      "source": [
        "### Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2wOLXAUVOV8",
        "outputId": "8867c865-36b6-4ef6-c652-c0249d8df724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "12.1\n",
            "NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(torch.version.cuda)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\tprint(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCvxsgx4VOV8"
      },
      "source": [
        "## Data Declaration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2GvSSPqVOV8",
        "outputId": "c8d847fb-97bc-4aa1-b411-c2083a3ef21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space:  Box(0, 255, (96, 96, 3), uint8)\n",
            "Action space:  Discrete(5)\n"
          ]
        }
      ],
      "source": [
        "# Inspired by : https://github.com/pangyyen/carRacing-DeepRL/blob/main/ppo/ppo.ipynb\n",
        "\n",
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", domain_randomize=False, continuous=False, lap_complete_percent=0.95, max_episode_steps=10000)\n",
        "print(\"Observation space: \", env.observation_space) # (low, high, shape, dtype)\n",
        "print(\"Action space: \", env.action_space)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "observation, info = env.reset(seed=SEED)\n",
        "def show_animation():\n",
        "\tshow_animation_frames(env.render())\n",
        "\n",
        "def show_animation_frames(frames):\n",
        "\tfig = plt.figure(figsize=(7, 5))\n",
        "\tplt.axis('off')\n",
        "\tim = plt.imshow(frames[0])\n",
        "\n",
        "\tdef animate(i):\n",
        "\t\tim.set_data(frames[i])\n",
        "\t\treturn im,\n",
        "\n",
        "\tanim = animation.FuncAnimation(fig, animate, frames=len(frames), repeat=False)\n",
        "\tplt.close(fig)\n",
        "\tdisplay(HTML(anim.to_jshtml()))\n",
        "\n",
        "def show_current_frame(env, data):\n",
        "\tframe = env.render()\n",
        "\tfig, _ = plt.subplots()\n",
        "\tr = fig.canvas.get_renderer()\n",
        "\tplt.imshow(frame)\n",
        "\tplt.axis('off')\n",
        "\ttexts = []\n",
        "\tsize_used = 0\n",
        "\tfor i, key in enumerate(data):\n",
        "\t\ttext = plt.text(0, 0, f'{key}: {data[key]}', fontsize=12, color='black', backgroundcolor='white', ha=\"center\")\n",
        "\t\tsize_used += text.get_window_extent(renderer=r).width\n",
        "\t\ttexts.append(text)\n",
        "\tsplit = (700 - size_used) / (len(data) + 1)\n",
        "\tnext_position = split\n",
        "\tfor t in texts:\n",
        "\t\tt.set_position((next_position, 0))\n",
        "\t\tnext_position = next_position + t.get_window_extent(renderer=r).width + split\n",
        "\tclear_output(wait=True)\n",
        "\tplt.show()\n",
        "\n",
        "def skip_zooming(env):\n",
        "\tno_action = 0\n",
        "\tif type(env.action_space) != gym.spaces.Discrete:\n",
        "\t\tno_action = np.zeros((env.action_space.shape[0]))\n",
        "\n",
        "\tfor i in range(50):\n",
        "\t\tobservation, _, terminated, truncated, info = env.step(no_action)\n",
        "\n",
        "\t\tif terminated or truncated:\n",
        "\t\t\tobservation, info = env.reset()\n",
        "\t\t\tbreak\n",
        "\treturn observation, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYfc-8GFVOV9"
      },
      "source": [
        "### Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KSkwQhR8VOV9"
      },
      "outputs": [],
      "source": [
        "transform = T.Compose([\n",
        "\tT.ToPILImage(),\n",
        "\tT.Grayscale(num_output_channels=1),\n",
        "\tT.Resize((84, 84)),\n",
        "\tT.ToTensor(),\n",
        "\tT.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XooYEygsVOV9"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVCgGGoVOV9"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6co9R30VVOV9"
      },
      "source": [
        "#### Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIRvRtvxVOV9"
      },
      "source": [
        "DQN is at its heart Q-Learning using Deep Neural Networks to predict the behavior of its environment and to predict which action is the best.\n",
        "\n",
        "Our goal, when implementing DQN is to maximize the rewards of our policy $\\pi^{*}$ described as followed, where $Q^{*}$ is defined as the optimal action-value function.\n",
        "\n",
        "$$\n",
        "\\pi^{*}(s) = \\underset{a}{\\arg\\max} \\; Q^{*}(s,a)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "The definition of $Q^{*}$ follows the Bellman Optimality Equation:\n",
        "\n",
        "$$\n",
        "Q^{*}(s,a) = \\mathbb{E} \\left[ r + \\gamma \\underset{a'}{\\max} Q^{*}(s', a') \\; | \\; s, a \\right]\n",
        "$$\n",
        "\n",
        "The equation means that the value of an action is dictated by the current reward + the best reward we can get from the best next action. The $\\gamma$ symbol is used only so that we can diminishes the importance of futur action on the long run.\n",
        "\n",
        "Our goal is to maximize the rewards we will have on the long term, which can be defined as:\n",
        "\n",
        "$$\n",
        "G_t = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\gamma^{3} r_{t+3} + ...  \n",
        "$$\n",
        "\n",
        "Based on Bellman's Optimality Equation, we are able to use the following update equation:\n",
        "\n",
        "$$\n",
        "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\underset{a'}{\\max}Q(s', a') - Q(s,a) \\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZunLW1MVOV9"
      },
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Jd0qR4DEVOV9"
      },
      "outputs": [],
      "source": [
        "# Implementation based on : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def append(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, n_actions):\n",
        "    \"\"\"\n",
        "    Q-Network made of a Deep neural network\n",
        "    \"\"\"\n",
        "    super(DQN, self).__init__()\n",
        "    # TODO: Ajust the depth of the model so that we don't need to use 128 each time,\n",
        "    # and evaluate the impact of changing those values\n",
        "    self.net = nn.Sequential(\n",
        "      # Adjusted for RGB input (96x96x3) without resizing\n",
        "      nn.Conv2d(1, 32, kernel_size=8, stride=4),    # Output: 32x23x23\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, kernel_size=4, stride=2),   # Output: 64x10x10\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1),    # Output: 64x8x8\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(64 * 7 * 7, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class DQNAgent():\n",
        "  def __init__(self, env):\n",
        "    \"\"\"\n",
        "    Agent made of DQNs used for learning how to use the sim racer.\n",
        "    \"\"\"\n",
        "    # TODO : make it so that it's possible to verify which hyperparameter was the best\n",
        "    # TODO : we will handle the images as greyscale because we don't need\n",
        "    # to handle the colors, it doesn't add that much information more than greyscale\n",
        "    # TODO : inclure les formules mathÃ©matiques\n",
        "\n",
        "    # Hyperparameters\n",
        "    self.GAMMA = 0.99\n",
        "    self.LR = 3e-4\n",
        "    self.BATCH_SIZE = 64\n",
        "    self.MEMORY_SIZE = 10000\n",
        "    self.EPSILON_START = 1.0\n",
        "    self.EPSILON_END = 0.01\n",
        "    self.EPSILON_DECAY = 1000\n",
        "    self.TARGET_UPDATE_FREQ = 10\n",
        "\n",
        "    # Possible actions\n",
        "    self.discrete_actions = [\n",
        "      0,\t# Do nothing\n",
        "      1,\t# Steer left\n",
        "      2,\t# Steer right\n",
        "      3,\t# Accelerate\n",
        "      4,\t# Brake\n",
        "    ]\n",
        "    n_actions = len(self.discrete_actions)\n",
        "\n",
        "    # Neural Network Declarations Here\n",
        "    self.policy_net = DQN(n_actions).to(device)\n",
        "    self.target_net = DQN(n_actions).to(device)\n",
        "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
        "    self.memory = ReplayMemory(10000)\n",
        "\n",
        "    self.steps_done = 0\n",
        "    self.episode_durations = []\n",
        "\n",
        "  def select_action(self, state):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy strategy\n",
        "\n",
        "    state: contains the rgb image of the car and the racing track (96, 96, 3)\n",
        "    \"\"\"\n",
        "    sample = random.random()\n",
        "    eps_threshold = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * \\\n",
        "      math.exp(-1. * self.steps_done / self.EPSILON_DECAY)\n",
        "    self.steps_done += 1\n",
        "\n",
        "\n",
        "    # print(f\"{eps_threshold}\")\n",
        "    if sample > eps_threshold:\n",
        "      with torch.no_grad():\n",
        "        # Add batch dimension and convert to float\n",
        "        state_tensor = state.unsqueeze(0).float().to(device)\n",
        "        q_values = self.policy_net(state_tensor)\n",
        "        action_idx = q_values.argmax().item()\n",
        "    else:\n",
        "      action_idx = random.randint(0, len(self.discrete_actions)-1)\n",
        "\n",
        "    return action_idx\n",
        "\n",
        "  def preprocess_state(self, state):\n",
        "    # Convert numpy array to tensor and normalize to [0,1]\n",
        "    # Input shape: (96, 96, 3) -> Output shape: (3, 96, 96)\n",
        "    return torch.from_numpy(state).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "  def optimize_model(self):\n",
        "    \"\"\"\n",
        "    Apply the backward propagation to the policy_net and the target_net.\n",
        "    \"\"\"\n",
        "    if len(self.memory) < self.BATCH_SIZE:\n",
        "      return\n",
        "\n",
        "    transitions = self.memory.sample(self.BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    states = torch.stack([s for s in batch.state]).to(device)\n",
        "    actions = torch.tensor(batch.action, dtype=torch.long, device=device)\n",
        "    rewards = torch.tensor(batch.reward, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "    dones = torch.tensor(batch.done, dtype=torch.bool, device=device).unsqueeze(1)\n",
        "    next_states = torch.stack([s for s in batch.next_state]).to(device)\n",
        "\n",
        "    # Remove flattening steps to keep spatial structure\n",
        "    current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "    next_q = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "    expected_q = rewards + (self.GAMMA * next_q * ~dones)\n",
        "\n",
        "    # loss = nn.MSELoss()(current_q, expected_q)\n",
        "    loss = nn.SmoothL1Loss()(current_q, expected_q)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 10)\n",
        "    self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teWHBJfaVOV9"
      },
      "source": [
        "#### Training Agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "\t\"seed\": 42,\n",
        "\t\"max_episodes\": 300000,\n",
        "\t\"max_timesteps\": 10000,\n",
        "\t\"max_losing_step\": 75,\n",
        "\t\"is_log_std\": True,\n",
        "\t\"lr_actor\": 3e-4,\n",
        "\t\"actor_std\": 0.2,\n",
        "\t\"actor_embeddings\": [1024, 512],\n",
        "\t\"lr_critic\": 3e-4,\n",
        "\t\"critic_embeddings\": [1024, 512],\n",
        "\t\"gamma\": 0.99,\n",
        "\t\"eps_clip\": 0.2,\n",
        "\t\"k_epochs\": 10,\n",
        "\t\"continuous\": False,\n",
        "\t\"lap_complete_percent\": 0.95,\n",
        "\t\"stop_criteria_count\": 10\n",
        "}"
      ],
      "metadata": {
        "id": "eN27iuFcjdrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "iljGLxzoVOV9",
        "outputId": "bff924d1-ba43-481e-cd99-ec56d0d99799"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e95680035b41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m#       show_current_frame(env, {\"Episode\": episode, \"Timestep\": t})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-aee7789a0f6a>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Remove flattening steps to keep spatial structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mcurrent_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mnext_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mexpected_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-aee7789a0f6a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with wandb.init(\n",
        "\t\tconfig=config,\n",
        "\t\tproject='INF8225 - TP4',\n",
        "\t\tgroup='PPO',\n",
        "\t\tsave_code=True,\n",
        "\t):\n",
        "\t# Training loop\n",
        "\tagent = DQNAgent(env)\n",
        "\n",
        "\t# Real progress starts at 100\n",
        "\tn_episode = 700\n",
        "\tfor episode in range(n_episode):\n",
        "\t\tstate, _ = env.reset(seed=SEED)\n",
        "\t\tstate, _ = skip_zooming(env)\n",
        "\t\tstate = transform(state)\n",
        "\t\ttotal_reward = 0\n",
        "\t\trewards = []\n",
        "\t\tdone = False\n",
        "\t\tt = 0\n",
        "\n",
        "\t\twhile not done:\n",
        "\t\t\taction_idx = agent.select_action(state)\n",
        "\t\t\tnext_state, reward, done, truncated, _ = env.step(action_idx)\n",
        "\t\t\tdone = done or truncated\n",
        "\n",
        "\t\t\tnext_state = transform(next_state)\n",
        "\n",
        "\t\t\tagent.memory.append(state, action_idx, next_state, reward, done)\n",
        "\t\t\trewards.append(reward)\n",
        "\t\t\tstate = next_state\n",
        "\t\t\ttotal_reward += reward\n",
        "\n",
        "\t\t\tif len(rewards) > 75:\n",
        "\t\t\t\t# todo\n",
        "\t\t\t\tpass\n",
        "\n",
        "\n",
        "\t\t\t# if t % 20 == 0:\n",
        "\t\t\t# \tshow_current_frame(env, {\"Episode\": episode, \"Timestep\": t})\n",
        "\n",
        "\t\t\tagent.optimize_model()\n",
        "\n",
        "\t\t\tt += 1\n",
        "\n",
        "\t\tif episode % agent.TARGET_UPDATE_FREQ == 0:\n",
        "\t\t\tagent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "\t\tprint(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRqpJaHEVOV9"
      },
      "source": [
        "#### Car Racing Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTv24Jv9VOV9"
      },
      "outputs": [],
      "source": [
        "def run_agent_and_collect_frames(agent, env, seed=42):\n",
        "  state, _ = env.reset(seed=seed)\n",
        "  done = False\n",
        "  frames = []\n",
        "\n",
        "  while not done:\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "\n",
        "    preprocessed_state = agent.preprocess_state(state)\n",
        "    action = agent.select_action(preprocessed_state)\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    state = next_state\n",
        "\n",
        "    return frames\n",
        "\n",
        "# Function to display frames as an animation using matplotlib\n",
        "def show_animation_frames(frames):\n",
        "  fig = plt.figure(figsize=(7, 5))\n",
        "  plt.axis('off')\n",
        "  im = plt.imshow(frames[0])\n",
        "\n",
        "  def animate(i):\n",
        "    im.set_data(frames[i])\n",
        "    return im,\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, repeat=False)\n",
        "  plt.close(fig)\n",
        "  display(HTML(anim.to_jshtml()))\n",
        "\n",
        "# Run the episode with the trained agent\n",
        "frames = run_agent_and_collect_frames(agent, env)\n",
        "\n",
        "# Show the animation\n",
        "show_animation_frames(frames)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}